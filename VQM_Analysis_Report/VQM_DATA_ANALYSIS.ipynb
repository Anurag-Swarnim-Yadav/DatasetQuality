{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VQM DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modified file has been saved to: ./vrepair_non_domain_data/non_domain_train_NO_CWE.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './vrepair_non_domain_data/processed_non_domain_train.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove all occurrences of \"CWE-XXX\" or \"CWE-XX\" from the 'source' column\n",
    "# This regex matches \"CWE-\" followed by two or three digits\n",
    "data['source'] = data['source'].str.replace(r'CWE-\\d{2,3}', '', regex=True)\n",
    "\n",
    "# Define the path for the new CSV file without CWE codes in 'source'\n",
    "new_file_path_no_cwe = './vrepair_non_domain_data/non_domain_train_NO_CWE.csv'\n",
    "\n",
    "# Save the modified DataFrame to the new CSV file\n",
    "data.to_csv(new_file_path_no_cwe, index=False)\n",
    "\n",
    "print(f\"The modified file has been saved to: {new_file_path_no_cwe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modified file has been saved to: ./vrepair_non_domain_data/non_domain_val_NO_CWE.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './vrepair_non_domain_data/processed_non_domain_val.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove all occurrences of \"CWE-XXX\" or \"CWE-XX\" from the 'source' column\n",
    "# This regex matches \"CWE-\" followed by two or three digits\n",
    "data['source'] = data['source'].str.replace(r'CWE-\\d{2,3}', '', regex=True)\n",
    "\n",
    "# Define the path for the new CSV file without CWE codes in 'source'\n",
    "new_file_path_no_cwe = './vrepair_non_domain_data/non_domain_val_NO_CWE.csv'\n",
    "\n",
    "# Save the modified DataFrame to the new CSV file\n",
    "data.to_csv(new_file_path_no_cwe, index=False)\n",
    "\n",
    "print(f\"The modified file has been saved to: {new_file_path_no_cwe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entries in the 'source' column: 3943\n",
      "Duplicate entries in the 'source' column: 17303\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './vrepair_non_domain_data/non_domain_train_NO_CWE.csv'  # Make sure to replace this with the actual path to your file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Count unique entries in the 'source' column\n",
    "data['source'] = data['source'].str.strip().str.lower()\n",
    "unique_sources_count = data['source'].nunique()\n",
    "\n",
    "# Calculate the number of duplicate entries\n",
    "duplicates_count = len(data) - unique_sources_count\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Unique entries in the 'source' column: {unique_sources_count}\")\n",
    "print(f\"Duplicate entries in the 'source' column: {duplicates_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entries in the 'source' column: 1695\n",
      "Duplicate entries in the 'source' column: 666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './vrepair_non_domain_data/non_domain_val_NO_CWE.csv'  # Make sure to replace this with the actual path to your file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Count unique entries in the 'source' column\n",
    "data['source'] = data['source'].str.strip().str.lower()\n",
    "unique_sources_count = data['source'].nunique()\n",
    "\n",
    "# Calculate the number of duplicate entries\n",
    "duplicates_count = len(data) - unique_sources_count\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Unique entries in the 'source' column: {unique_sources_count}\")\n",
    "print(f\"Duplicate entries in the 'source' column: {duplicates_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS FOR TRAIN IN NON-DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./vrepair_non_domain_data/non_domain_train_NO_CWE.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_StartBug> ' in row:\n",
    "        return row.replace('<S2SV_StartBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./vrepair_non_domain_data/non_domain_train_NO_CWE_No_Start.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./vrepair_non_domain_data/non_domain_train_NO_CWE_No_Start.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_EndBug> ' in row:\n",
    "        return row.replace('<S2SV_EndBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./vrepair_non_domain_data/non_domain_train_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS FOR THE VAL IN NON-DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./vrepair_non_domain_data/non_domain_val_NO_CWE.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_StartBug> ' in row:\n",
    "        return row.replace('<S2SV_StartBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./vrepair_non_domain_data/non_domain_val_NO_CWE_No_Start.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./vrepair_non_domain_data/non_domain_val_NO_CWE_No_Start.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_EndBug> ' in row:\n",
    "        return row.replace('<S2SV_EndBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./vrepair_non_domain_data/non_domain_val_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entries in the 'source' column: 2624\n",
      "Duplicate entries in the 'source' column: 18622\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './vrepair_non_domain_data/non_domain_train_final.csv'  # Make sure to replace this with the actual path to your file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Count unique entries in the 'source' column\n",
    "unique_sources_count = data['source'].nunique()\n",
    "\n",
    "# Calculate the number of duplicate entries\n",
    "duplicates_count = len(data) - unique_sources_count\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Unique entries in the 'source' column: {unique_sources_count}\")\n",
    "print(f\"Duplicate entries in the 'source' column: {duplicates_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entries in the 'source' column: 1579\n",
      "Duplicate entries in the 'source' column: 782\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './vrepair_non_domain_data/non_domain_val_final.csv'  # Make sure to replace this with the actual path to your file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Count unique entries in the 'source' column\n",
    "unique_sources_count = data['source'].nunique()\n",
    "\n",
    "# Calculate the number of duplicate entries\n",
    "duplicates_count = len(data) - unique_sources_count\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Unique entries in the 'source' column: {unique_sources_count}\")\n",
    "print(f\"Duplicate entries in the 'source' column: {duplicates_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE AN UNIQUE FILES NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique rows based on the 'source' column: 2624\n",
      "New CSV with unique rows has been created at './vrepair_non_domain_data/final/non_domain_train.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def write_unique_rows_and_count(input_csv, output_csv, unique_column):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Drop duplicate rows based on the unique_column\n",
    "    unique_df = df.drop_duplicates(subset=[unique_column])\n",
    "\n",
    "    # Count the number of unique rows\n",
    "    unique_count = unique_df.shape[0]\n",
    "\n",
    "    # Write the DataFrame with unique rows to a new CSV file\n",
    "    unique_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Print the number of unique rows and confirmation of file creation\n",
    "    print(f\"Number of unique rows based on the '{unique_column}' column: {unique_count}\")\n",
    "    print(f\"New CSV with unique rows has been created at '{output_csv}'.\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = './vrepair_non_domain_data/non_domain_train_final.csv'  # Replace with your input CSV file path\n",
    "output_csv_path = './vrepair_non_domain_data/final/non_domain_train.csv'  # Replace with your desired output CSV file path\n",
    "unique_column_name = 'source'  # Column based on which to filter unique rows\n",
    "\n",
    "write_unique_rows_and_count(input_csv_path, output_csv_path, unique_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique rows based on the 'source' column: 1579\n",
      "New CSV with unique rows has been created at './vrepair_non_domain_data/final/non_domain_val.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def write_unique_rows_and_count(input_csv, output_csv, unique_column):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Drop duplicate rows based on the unique_column\n",
    "    unique_df = df.drop_duplicates(subset=[unique_column])\n",
    "\n",
    "    # Count the number of unique rows\n",
    "    unique_count = unique_df.shape[0]\n",
    "\n",
    "    # Write the DataFrame with unique rows to a new CSV file\n",
    "    unique_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Print the number of unique rows and confirmation of file creation\n",
    "    print(f\"Number of unique rows based on the '{unique_column}' column: {unique_count}\")\n",
    "    print(f\"New CSV with unique rows has been created at '{output_csv}'.\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = './vrepair_non_domain_data/non_domain_val_final.csv'  # Replace with your input CSV file path\n",
    "output_csv_path = './vrepair_non_domain_data/final/non_domain_val.csv'  # Replace with your desired output CSV file path\n",
    "unique_column_name = 'source'  # Column based on which to filter unique rows\n",
    "\n",
    "write_unique_rows_and_count(input_csv_path, output_csv_path, unique_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1579 matching entries.\n",
      "We have 0 unique entries written to ./vrepair_non_domain_data/final/no_val_in_train.csv.\n",
      "Here are the full rows:\n",
      "Empty DataFrame\n",
      "Columns: [cwe_id, source, target]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_remove_write_and_print_unique_entries(file1_path, file2_path, output_path):\n",
    "    try:\n",
    "        # Load both CSV files\n",
    "        data1 = pd.read_csv(file1_path)\n",
    "        data2 = pd.read_csv(file2_path)\n",
    "        \n",
    "        # Ensure 'source' column is present in both dataframes\n",
    "        if 'source' not in data1.columns or 'source' not in data2.columns:\n",
    "            print(\"Error: 'source' column is missing in one or both of the files.\")\n",
    "            return\n",
    "        \n",
    "        # Normalize 'source' columns by stripping whitespace and converting to lowercase\n",
    "        data1['source'] = data1['source'].str.strip().str.lower()\n",
    "        data2['source'] = data2['source'].str.strip().str.lower()\n",
    "\n",
    "        # Extract 'source' column for comparison (use a set for faster membership testing)\n",
    "        sources2 = set(data2['source'])\n",
    "        \n",
    "        # Filter rows in data1 that have their 'source' in data2\n",
    "        matching_rows = data1[data1['source'].isin(sources2)]\n",
    "\n",
    "        # Remove matching entries from data1 (those in sources2)\n",
    "        unique_rows = data1[~data1['source'].isin(sources2)]\n",
    "        \n",
    "        # Write the unique rows to a new CSV file\n",
    "        unique_rows.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Print details\n",
    "        print(f\"Found {len(matching_rows)} matching entries.\")\n",
    "        print(f\"We have {len(unique_rows)} unique entries written to {output_path}.\")\n",
    "        print(\"Here are the full rows:\")\n",
    "        print(unique_rows)\n",
    "        \n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error parsing the files. Check the delimiter and file format.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Ensure the file paths are correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Specify paths to your files\n",
    "file1_path = './vrepair_non_domain_data/final/non_domain_val.csv'  # Replace this with your first CSV file path\n",
    "file2_path = './vrepair_non_domain_data/final/non_domain_train.csv'  # Replace this with your second CSV file path\n",
    "output_path = './vrepair_non_domain_data/final/no_val_in_train.csv'  # Replace with the desired path for the output CSV file\n",
    "\n",
    "# Execute the function\n",
    "find_remove_write_and_print_unique_entries(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LETS US CONSIDER THE VULNERABILITY DATASET NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./cve_fixes_and_big_vul/test.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_StartBug> ' in row:\n",
    "        return row.replace('<S2SV_StartBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./cve_fixes_and_big_vul/test_nostart.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./cve_fixes_and_big_vul/test_nostart.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_EndBug> ' in row:\n",
    "        return row.replace('<S2SV_EndBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./cve_fixes_and_big_vul/test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./cve_fixes_and_big_vul/val.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_StartBug> ' in row:\n",
    "        return row.replace('<S2SV_StartBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./cve_fixes_and_big_vul/val_nostart.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./cve_fixes_and_big_vul/val_nostart.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_EndBug> ' in row:\n",
    "        return row.replace('<S2SV_EndBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./cve_fixes_and_big_vul/val_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./cve_fixes_and_big_vul/train.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_StartBug> ' in row:\n",
    "        return row.replace('<S2SV_StartBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./cve_fixes_and_big_vul/train_nostart.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a CSV file for example)\n",
    "# Replace 'your_file.csv' with the actual file path\n",
    "df = pd.read_csv('./cve_fixes_and_big_vul/train_nostart.csv')\n",
    "\n",
    "# Function to replace <S2SV_StartBug> in each row of the 'source' column\n",
    "def replace_bug_marker(row):\n",
    "    if '<S2SV_EndBug> ' in row:\n",
    "        return row.replace('<S2SV_EndBug> ', '')\n",
    "    return row\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "df['source'] = df['source'].apply(replace_bug_marker)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or display\n",
    "df.to_csv('./cve_fixes_and_big_vul/train_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS FOR CROSSFILE BETWEEN VULNERABILITY DATASET AND BUGFIX DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 511 matching entries.\n",
      "We have 579 unique entries written to ./cve_fixes_and_big_vul/final/test_removed_non_domain.csv.\n",
      "Here are the full rows:\n",
      "      index   cwe_id                                             source  \\\n",
      "1      7632  CWE-190  jpc_streamlist_t * jpc_ppmstabtostreams ( jpc_...   \n",
      "2       568   CWE-59  static int mountpoint_last ( struct nameidata ...   \n",
      "3      8359  CWE-362  static struct rtable * icmp_route_lookup ( str...   \n",
      "4      6344  CWE-772  generic_ret * init_2_svc ( krb5_ui_4 * arg , s...   \n",
      "5       438  CWE-416  int ppp_register_net_channel ( struct net * ne...   \n",
      "...     ...      ...                                                ...   \n",
      "1081   2550  CWE-189  static int snd_ctl_elem_add ( struct snd_ctl_f...   \n",
      "1082   8119  CWE-295  int options_cmdline ( char * arg1 , char * arg...   \n",
      "1087   7052  CWE-476  static gf_av1config * av1_duplicateconfig ( gf...   \n",
      "1088   7836   CWE-20  error_t rawsocketreceiveippacket ( socket * so...   \n",
      "1089   6170  CWE-772  static image * readmatimage ( const imageinfo ...   \n",
      "\n",
      "                                                 target  \\\n",
      "1     <S2SV_ModStart> * streams ; jas_uchar <S2SV_Mo...   \n",
      "2     <S2SV_ModStart> -> mnt = <S2SV_ModEnd> nd -> p...   \n",
      "3     <S2SV_ModStart> -> replyopts . opt . opt . <S2...   \n",
      "4     <S2SV_ModStart> ; gss_buffer_desc client_name ...   \n",
      "5     <S2SV_ModStart> -> chan_net = get_net ( net ) ...   \n",
      "...                                                 ...   \n",
      "1081  <S2SV_ModStart> ; if ( info -> count < 1 ) ret...   \n",
      "1082  <S2SV_ModStart> NULL , NULL , NULL <S2SV_ModSt...   \n",
      "1087                                                 \\n   \n",
      "1088  <S2SV_ModStart> ; message -> interface = queue...   \n",
      "1089  <S2SV_ModStart> ) ; } quantum_info = ( Quantum...   \n",
      "\n",
      "                                  project_and_commit_id          cve_id  \\\n",
      "1     mdadams@jasper/d42b2388f7f8e0332c846675133acea...   CVE-2016-9557   \n",
      "2     torvalds@linux/295dc39d941dc2ae53d5c170365af4c...   CVE-2014-5045   \n",
      "3     torvalds@linux/f6d8bd051c391c1c0458a30b2a7abcd...   CVE-2012-3552   \n",
      "4     krb5@krb5/83ed75feba32e46f736fcce0d96a0445f29b...   CVE-2015-8631   \n",
      "5     torvalds@linux/1f461dcdd296eecedaffffc6bae2bfa...   CVE-2016-4805   \n",
      "...                                                 ...             ...   \n",
      "1081  torvalds@linux/82262a46627bebb0febcc26664746c2...   CVE-2014-4655   \n",
      "1082  mtrojnar@stunnel/ebad9ddc4efb2635f37174c9d800d...  CVE-2021-20230   \n",
      "1087  gpac@gpac/b2eab95e07cb5819375a50358d4806a8813b...  CVE-2021-31262   \n",
      "1088  Oryx-Embedded@CycloneTCP/de5336016edbe1e90327d...  CVE-2021-26788   \n",
      "1089  ImageMagick@ImageMagick/79e5dbcdd1fc2f714f9bae...  CVE-2017-13146   \n",
      "\n",
      "                                       original_address               time  \\\n",
      "1     https://github.com/mdadams/jasper/commit/d42b2...  2017-03-23T18:59Z   \n",
      "2     https://github.com/torvalds/linux/commit/295dc...  2014-08-01T11:13Z   \n",
      "3     https://github.com/torvalds/linux/commit/f6d8b...  2012-10-03T11:02Z   \n",
      "4     https://github.com/krb5/krb5/commit/83ed75feba...  2016-02-13T02:59Z   \n",
      "5     https://github.com/torvalds/linux/commit/1f461...  2016-05-23T10:59Z   \n",
      "...                                                 ...                ...   \n",
      "1081  https://github.com/torvalds/linux/commit/82262...  2014-07-03T04:22Z   \n",
      "1082  https://github.com/mtrojnar/stunnel/commit/eba...  2021-02-23T17:15Z   \n",
      "1087  https://github.com/gpac/gpac/commit/b2eab95e07...  2021-04-19T19:15Z   \n",
      "1088  https://github.com/Oryx-Embedded/CycloneTCP/co...  2021-03-08T13:15Z   \n",
      "1089  https://github.com/ImageMagick/ImageMagick/com...  2017-08-23T06:29Z   \n",
      "\n",
      "                                    localization_target  \n",
      "1       <S2SV_StartBug> uchar * dataptr ; <S2SV_EndBug>  \n",
      "2     <S2SV_StartBug> path -> mnt = mntget ( nd -> p...  \n",
      "3     <S2SV_StartBug> . daddr = ( param -> replyopts...  \n",
      "4     <S2SV_StartBug> gss_buffer_desc client_name , ...  \n",
      "5     <S2SV_StartBug> pch -> chan_net = net ; <S2SV_...  \n",
      "...                                                 ...  \n",
      "1081  <S2SV_StartBug> if ( ! replace && card -> user...  \n",
      "1082  <S2SV_StartBug> parse_global_option ( CMD_PRIN...  \n",
      "1087                                                NaN  \n",
      "1088  <S2SV_StartBug> message -> srcIpAddr = queueIt...  \n",
      "1089  <S2SV_StartBug> } <S2SV_EndBug> <S2SV_StartBug...  \n",
      "\n",
      "[579 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_remove_write_and_print_unique_entries(file1_path, file2_path, output_path):\n",
    "    try:\n",
    "        # Load both CSV files\n",
    "        data1 = pd.read_csv(file1_path)\n",
    "        data2 = pd.read_csv(file2_path)\n",
    "        \n",
    "        # Ensure 'source' column is present in both dataframes\n",
    "        if 'source' not in data1.columns or 'source' not in data2.columns:\n",
    "            print(\"Error: 'source' column is missing in one or both of the files.\")\n",
    "            return\n",
    "        \n",
    "        # Normalize 'source' columns by stripping whitespace and converting to lowercase\n",
    "        data1['source'] = data1['source'].str.strip().str.lower()\n",
    "        data2['source'] = data2['source'].str.strip().str.lower()\n",
    "\n",
    "        # Extract 'source' column for comparison (use a set for faster membership testing)\n",
    "        sources2 = set(data2['source'])\n",
    "        \n",
    "        # Filter rows in data1 that have their 'source' in data2\n",
    "        matching_rows = data1[data1['source'].isin(sources2)]\n",
    "\n",
    "        # Remove matching entries from data1 (those in sources2)\n",
    "        unique_rows = data1[~data1['source'].isin(sources2)]\n",
    "        \n",
    "        # Write the unique rows to a new CSV file\n",
    "        unique_rows.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Print details\n",
    "        print(f\"Found {len(matching_rows)} matching entries.\")\n",
    "        print(f\"We have {len(unique_rows)} unique entries written to {output_path}.\")\n",
    "        print(\"Here are the full rows:\")\n",
    "        print(unique_rows)\n",
    "        \n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error parsing the files. Check the delimiter and file format.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Ensure the file paths are correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Specify paths to your files\n",
    "file1_path = './cve_fixes_and_big_vul/test_NoTag_final.csv'  # Replace this with your first CSV file path\n",
    "file2_path = './vrepair_non_domain_data/final/non_domain_train.csv'  # Replace this with your second CSV file path\n",
    "output_path = './cve_fixes_and_big_vul/final/test_removed_non_domain.csv'  # Replace with the desired path for the output CSV file\n",
    "\n",
    "# Execute the function\n",
    "find_remove_write_and_print_unique_entries(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 243 matching entries.\n",
      "We have 293 unique entries written to ./cve_fixes_and_big_vul/final/val_removed_non_domain.csv.\n",
      "Here are the full rows:\n",
      "     index   cwe_id                                             source  \\\n",
      "0     3667  CWE-119  bool f2fs_init_extent_tree ( struct inode * in...   \n",
      "1     7005  CWE-400  static void listdir ( unsigned int depth , int...   \n",
      "2     8415  CWE-125  int mutt_seqset_iterator_next ( struct seqseti...   \n",
      "5     3305  CWE-119  static int wdm_post_reset ( struct usb_interfa...   \n",
      "6       65  CWE-362  static int userfaultfd_register ( struct userf...   \n",
      "..     ...      ...                                                ...   \n",
      "528   2487  CWE-000  static int unix_dgram_connect ( struct socket ...   \n",
      "529   4292  CWE-362  static void clear_evtchn_to_irq_row ( unsigned...   \n",
      "532   6185  CWE-639  int ca_validate_pubkey ( struct iked * env , s...   \n",
      "533   7874   CWE-20  error_t enc624j600updatemacaddrfilter ( netint...   \n",
      "535   6524  CWE-119  cjson * cjson_createfloatarray ( double * numb...   \n",
      "\n",
      "                                                target  \\\n",
      "0    <S2SV_ModStart> i_ext ) { bool ret = __f2fs_in...   \n",
      "1    <S2SV_ModStart> PureFileInfo * r ; char * allo...   \n",
      "2    <S2SV_ModStart> return 1 ; iter -> substr_end ...   \n",
      "5    <S2SV_ModStart> ; int rv ; clear_bit ( WDM_OVE...   \n",
      "6    <S2SV_ModStart> -> mmap_sem ) ; if ( ! mmget_s...   \n",
      "..                                                 ...   \n",
      "528  <S2SV_ModStart> = other ; unix_dgram_peer_wake...   \n",
      "529  <S2SV_ModStart> col ++ ) WRITE_ONCE ( <S2SV_Mo...   \n",
      "532  <S2SV_ModStart> ( peerkey && <S2SV_ModEnd> EVP...   \n",
      "533  <S2SV_ModStart> ( interface , ENC624J600_EHT1 ...   \n",
      "535  <S2SV_ModStart> * cJSON_CreateFloatArray ( con...   \n",
      "\n",
      "                                 project_and_commit_id          cve_id  \\\n",
      "0    torvalds@linux/dad48e73127ba10279ea33e6dbc8d39...  CVE-2017-18193   \n",
      "1    jedisct1@pure-ftpd/aea56f4bcb9948d456f3fae4d04...  CVE-2019-20176   \n",
      "2    neomutt@neomutt/fa1db5785e5cfd9d3cd27b7571b9fe...  CVE-2021-32055   \n",
      "5    torvalds@linux/c0f5ecee4e741667b2493c742b60b62...   CVE-2013-1860   \n",
      "6    torvalds@linux/04f5866e41fb70690e28397487d8bd8...  CVE-2019-11599   \n",
      "..                                                 ...             ...   \n",
      "528  torvalds@linux/7d267278a9ece963d77eefec6163022...   CVE-2013-7446   \n",
      "529  torvalds@linux/073d0552ead5bfc7a3a9c01de590e92...  CVE-2020-27675   \n",
      "532  openbsd@src/7afb2d41c6d373cf965285840b85c45011...  CVE-2020-16088   \n",
      "533  Oryx-Embedded@CycloneTCP/de5336016edbe1e90327d...  CVE-2021-26788   \n",
      "535  esnet@iperf/91f2fa59e8ed80dfbf400add0164ee0e50...   CVE-2016-4303   \n",
      "\n",
      "                                      original_address               time  \\\n",
      "0    https://github.com/torvalds/linux/commit/dad48...  2018-02-22T15:29Z   \n",
      "1    https://github.com/jedisct1/pure-ftpd/commit/a...  2019-12-31T15:15Z   \n",
      "2    https://github.com/neomutt/neomutt/commit/fa1d...  2021-05-05T16:15Z   \n",
      "5    https://github.com/torvalds/linux/commit/c0f5e...  2013-03-22T11:59Z   \n",
      "6    https://github.com/torvalds/linux/commit/04f58...  2019-04-29T18:29Z   \n",
      "..                                                 ...                ...   \n",
      "528  https://github.com/torvalds/linux/commit/7d267...  2015-12-28T11:59Z   \n",
      "529  https://github.com/torvalds/linux/commit/073d0...  2020-10-22T21:15Z   \n",
      "532  https://github.com/openbsd/src/commit/7afb2d41...  2020-07-28T12:15Z   \n",
      "533  https://github.com/Oryx-Embedded/CycloneTCP/co...  2021-03-08T13:15Z   \n",
      "535  https://github.com/esnet/iperf/commit/91f2fa59...  2016-09-26T14:59Z   \n",
      "\n",
      "                                   localization_target  \n",
      "0    <S2SV_StartBug> struct f2fs_sb_info * sbi = F2...  \n",
      "1    <S2SV_StartBug> PureFileInfo * r ; <S2SV_EndBu...  \n",
      "2    <S2SV_StartBug> while ( ! * ( iter -> substr_c...  \n",
      "5               <S2SV_StartBug> int rv ; <S2SV_EndBug>  \n",
      "6    <S2SV_StartBug> down_write ( & mm -> mmap_sem ...  \n",
      "..                                                 ...  \n",
      "528  <S2SV_StartBug> unix_state_double_unlock ( sk ...  \n",
      "529  <S2SV_StartBug> evtchn_to_irq [ row ] [ col ] ...  \n",
      "532  <S2SV_StartBug> if ( peerkey && ! EVP_PKEY_cmp...  \n",
      "533  <S2SV_StartBug> enc624j600WriteReg ( interface...  \n",
      "535  <S2SV_StartBug> cJSON * cJSON_CreateFloatArray...  \n",
      "\n",
      "[293 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_remove_write_and_print_unique_entries(file1_path, file2_path, output_path):\n",
    "    try:\n",
    "        # Load both CSV files\n",
    "        data1 = pd.read_csv(file1_path)\n",
    "        data2 = pd.read_csv(file2_path)\n",
    "        \n",
    "        # Ensure 'source' column is present in both dataframes\n",
    "        if 'source' not in data1.columns or 'source' not in data2.columns:\n",
    "            print(\"Error: 'source' column is missing in one or both of the files.\")\n",
    "            return\n",
    "        \n",
    "        # Normalize 'source' columns by stripping whitespace and converting to lowercase\n",
    "        data1['source'] = data1['source'].str.strip().str.lower()\n",
    "        data2['source'] = data2['source'].str.strip().str.lower()\n",
    "\n",
    "        # Extract 'source' column for comparison (use a set for faster membership testing)\n",
    "        sources2 = set(data2['source'])\n",
    "        \n",
    "        # Filter rows in data1 that have their 'source' in data2\n",
    "        matching_rows = data1[data1['source'].isin(sources2)]\n",
    "\n",
    "        # Remove matching entries from data1 (those in sources2)\n",
    "        unique_rows = data1[~data1['source'].isin(sources2)]\n",
    "        \n",
    "        # Write the unique rows to a new CSV file\n",
    "        unique_rows.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Print details\n",
    "        print(f\"Found {len(matching_rows)} matching entries.\")\n",
    "        print(f\"We have {len(unique_rows)} unique entries written to {output_path}.\")\n",
    "        print(\"Here are the full rows:\")\n",
    "        print(unique_rows)\n",
    "        \n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error parsing the files. Check the delimiter and file format.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Ensure the file paths are correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Specify paths to your files\n",
    "file1_path = './cve_fixes_and_big_vul/val_NoTag_final.csv'  # Replace this with your first CSV file path\n",
    "file2_path = './vrepair_non_domain_data/final/non_domain_train.csv'  # Replace this with your second CSV file path\n",
    "output_path = './cve_fixes_and_big_vul/final/val_removed_non_domain.csv'  # Replace with the desired path for the output CSV file\n",
    "\n",
    "# Execute the function\n",
    "find_remove_write_and_print_unique_entries(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1747 matching entries.\n",
      "We have 2044 unique entries written to ./cve_fixes_and_big_vul/final/train_removed_non_domain.csv.\n",
      "Here are the full rows:\n",
      "      index   cwe_id                                             source  \\\n",
      "0        25  CWE-000  static int kvm_vm_ioctl_set_pit ( struct kvm *...   \n",
      "6      4865  CWE-362  static long __mptctl_ioctl ( struct file * fil...   \n",
      "7      3341  CWE-264  syscall_define5 ( perf_event_open , struct per...   \n",
      "11     7847   CWE-20  error_t udpsenddatagram ( socket * socket , co...   \n",
      "12     5990  CWE-119  static inline void header_put_le_short ( sf_pr...   \n",
      "...     ...      ...                                                ...   \n",
      "3783   4567  CWE-125  static stmt_ty ast_for_funcdef_impl ( struct c...   \n",
      "3784   3021  CWE-399  void ptrace_triggered ( struct perf_event * bp...   \n",
      "3785    571  CWE-476  static int unimac_mdio_probe ( struct platform...   \n",
      "3786   2911  CWE-399  struct sctp_chunk * sctp_process_asconf ( stru...   \n",
      "3790   6236  CWE-000  static int tracedirective ( mastate * state , ...   \n",
      "\n",
      "                                                 target  \\\n",
      "0     <S2SV_ModStart> ps ) { int i ; <S2SV_ModStart>...   \n",
      "6     <S2SV_ModStart> return mptctl_getiocinfo ( ioc...   \n",
      "7     <S2SV_ModStart> -> ctx -> task <S2SV_ModEnd> !...   \n",
      "11    <S2SV_ModStart> * buffer ; NetInterface * inte...   \n",
      "12    <S2SV_ModStart> x ) { <S2SV_ModEnd> psf -> hea...   \n",
      "...                                                 ...   \n",
      "3783  <S2SV_ModStart> const node * n0 <S2SV_ModEnd> ...   \n",
      "3784  <S2SV_ModStart> perf_event * bp <S2SV_ModEnd> ...   \n",
      "3785  <S2SV_ModStart> 0 ) ; if ( ! r ) return - EINV...   \n",
      "3786  <S2SV_ModStart> asconf ) { sctp_addip_chunk_t ...   \n",
      "3790  <S2SV_ModStart> { option = ssplit <S2SV_ModEnd...   \n",
      "\n",
      "                                  project_and_commit_id          cve_id  \\\n",
      "0     torvalds@linux/0185604c2d82c560dab2f2933a18f79...   CVE-2015-7513   \n",
      "6     torvalds@linux/28d76df18f0ad5bcf5fa48510b225f0...  CVE-2020-12652   \n",
      "7     torvalds@linux/c3c87e770458aa004bd7ed3f29945ff...   CVE-2015-9004   \n",
      "11    Oryx-Embedded@CycloneTCP/de5336016edbe1e90327d...  CVE-2021-26788   \n",
      "12    erikd@libsndfile/708e996c87c5fae77b104ccfeb8f6...   CVE-2017-7586   \n",
      "...                                                 ...             ...   \n",
      "3783  python@typed_ast/156afcb26c198e162504a57caddfe...  CVE-2019-19275   \n",
      "3784  torvalds@linux/a8b0ca17b80e92faab46ee7179ba9e9...   CVE-2011-2918   \n",
      "3785  torvalds@linux/297a6961ffb8ff4dc66c9fbf53b924b...   CVE-2018-8043   \n",
      "3786  torvalds@linux/9de7922bc709eee2f609cd01d98aaed...   CVE-2014-3673   \n",
      "3790  embedthis@appweb/7e6a925f5e86a19a7934a94bbd695...   CVE-2014-9708   \n",
      "\n",
      "                                       original_address               time  \\\n",
      "0     https://github.com/torvalds/linux/commit/01856...  2016-02-08T03:59Z   \n",
      "6     https://github.com/torvalds/linux/commit/28d76...  2020-05-05T05:15Z   \n",
      "7     https://github.com/torvalds/linux/commit/c3c87...  2017-05-02T21:59Z   \n",
      "11    https://github.com/Oryx-Embedded/CycloneTCP/co...  2021-03-08T13:15Z   \n",
      "12    https://github.com/erikd/libsndfile/commit/708...  2017-04-07T20:59Z   \n",
      "...                                                 ...                ...   \n",
      "3783  https://github.com/python/typed_ast/commit/156...  2019-11-26T15:15Z   \n",
      "3784  https://github.com/torvalds/linux/commit/a8b0c...  2012-05-24T23:55Z   \n",
      "3785  https://github.com/torvalds/linux/commit/297a6...  2018-03-10T22:29Z   \n",
      "3786  https://github.com/torvalds/linux/commit/9de79...  2014-11-10T11:55Z   \n",
      "3790  https://github.com/embedthis/appweb/commit/7e6...  2015-03-31T14:59Z   \n",
      "\n",
      "                                    localization_target  \n",
      "0     <S2SV_StartBug> { <S2SV_EndBug> <S2SV_StartBug...  \n",
      "6     <S2SV_StartBug> return mptctl_getiocinfo ( arg...  \n",
      "7     <S2SV_StartBug> if ( group_leader -> ctx -> ty...  \n",
      "11    <S2SV_StartBug> NetTxAncillary ancillary ; <S2...  \n",
      "12    <S2SV_StartBug> { if ( psf -> headindex < SIGN...  \n",
      "...                                                 ...  \n",
      "3783  <S2SV_StartBug> ast_for_funcdef_impl ( struct ...  \n",
      "3784  <S2SV_StartBug> void ptrace_triggered ( struct...  \n",
      "3785  <S2SV_StartBug> priv -> base = devm_ioremap ( ...  \n",
      "3786  <S2SV_StartBug> sctp_addiphdr_t * hdr ; <S2SV_...  \n",
      "3790  <S2SV_StartBug> option = stok ( option , \"<S2S...  \n",
      "\n",
      "[2044 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_remove_write_and_print_unique_entries(file1_path, file2_path, output_path):\n",
    "    try:\n",
    "        # Load both CSV files\n",
    "        data1 = pd.read_csv(file1_path)\n",
    "        data2 = pd.read_csv(file2_path)\n",
    "        \n",
    "        # Ensure 'source' column is present in both dataframes\n",
    "        if 'source' not in data1.columns or 'source' not in data2.columns:\n",
    "            print(\"Error: 'source' column is missing in one or both of the files.\")\n",
    "            return\n",
    "        \n",
    "        # Normalize 'source' columns by stripping whitespace and converting to lowercase\n",
    "        data1['source'] = data1['source'].str.strip().str.lower()\n",
    "        data2['source'] = data2['source'].str.strip().str.lower()\n",
    "\n",
    "        # Extract 'source' column for comparison (use a set for faster membership testing)\n",
    "        sources2 = set(data2['source'])\n",
    "        \n",
    "        # Filter rows in data1 that have their 'source' in data2\n",
    "        matching_rows = data1[data1['source'].isin(sources2)]\n",
    "\n",
    "        # Remove matching entries from data1 (those in sources2)\n",
    "        unique_rows = data1[~data1['source'].isin(sources2)]\n",
    "        \n",
    "        # Write the unique rows to a new CSV file\n",
    "        unique_rows.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Print details\n",
    "        print(f\"Found {len(matching_rows)} matching entries.\")\n",
    "        print(f\"We have {len(unique_rows)} unique entries written to {output_path}.\")\n",
    "        print(\"Here are the full rows:\")\n",
    "        print(unique_rows)\n",
    "        \n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error parsing the files. Check the delimiter and file format.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Ensure the file paths are correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Specify paths to your files\n",
    "file1_path = './cve_fixes_and_big_vul/train_NoTag_final.csv'  # Replace this with your first CSV file path\n",
    "file2_path = './vrepair_non_domain_data/final/non_domain_train.csv'  # Replace this with your second CSV file path\n",
    "output_path = './cve_fixes_and_big_vul/final/train_removed_non_domain.csv'  # Replace with the desired path for the output CSV file\n",
    "\n",
    "# Execute the function\n",
    "find_remove_write_and_print_unique_entries(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My-Custom-Env",
   "language": "python",
   "name": "cwerepair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
